digraph {
	graph [size="44.55,44.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140478471530992 [label="
 (4, 3)" fillcolor=darkolivegreen1]
	140478453576752 [label=AddmmBackward0]
	140478453576896 -> 140478453576752
	140478486653824 [label="fc.bias
 (3)" fillcolor=lightblue]
	140478486653824 -> 140478453576896
	140478453576896 [label=AccumulateGrad]
	140478453576800 -> 140478453576752
	140478453576800 [label=ViewBackward0]
	140478453576608 -> 140478453576800
	140478453576608 [label=SqueezeBackward1]
	140478453577088 -> 140478453576608
	140478453577088 [label=MeanBackward1]
	140478453577232 -> 140478453577088
	140478453577232 [label=UnsqueezeBackward0]
	140478453577376 -> 140478453577232
	140478453577376 [label=ReluBackward0]
	140478453577520 -> 140478453577376
	140478453577520 [label=ConvolutionBackward0]
	140478453577664 -> 140478453577520
	140478453577664 [label=ConvolutionBackward0]
	140478453577952 -> 140478453577664
	140478453577952 [label=ReluBackward0]
	140478453578240 -> 140478453577952
	140478453578240 [label=ConvolutionBackward0]
	140478453578384 -> 140478453578240
	140478453578384 [label=ConvolutionBackward0]
	140478453578672 -> 140478453578384
	140478453578672 [label=ReluBackward0]
	140478453578960 -> 140478453578672
	140478453578960 [label=ConvolutionBackward0]
	140478453579104 -> 140478453578960
	140478453579104 [label=ConvolutionBackward0]
	140478453579152 -> 140478453579104
	140478453579152 [label=ReluBackward0]
	140478453578576 -> 140478453579152
	140478453578576 [label=ConvolutionBackward0]
	140478453578288 -> 140478453578576
	140478453578288 [label=ConvolutionBackward0]
	140478453577712 -> 140478453578288
	140478453577712 [label=ReluBackward0]
	140478453577136 -> 140478453577712
	140478453577136 [label=ConvolutionBackward0]
	140478453576848 -> 140478453577136
	140478453576848 [label=ConvolutionBackward0]
	140478453576272 -> 140478453576848
	140478453576272 [label=ReluBackward0]
	140478453575840 -> 140478453576272
	140478453575840 [label=ConvolutionBackward0]
	140478453734752 -> 140478453575840
	140478453734752 [label=ConvolutionBackward0]
	140478453734992 -> 140478453734752
	140478453734992 [label=ReluBackward0]
	140478453735280 -> 140478453734992
	140478453735280 [label=ConvolutionBackward0]
	140478453735376 -> 140478453735280
	140478453735376 [label=ConvolutionBackward0]
	140478453734944 -> 140478453735376
	140478453734944 [label=ReluBackward0]
	140478453633472 -> 140478453734944
	140478453633472 [label=ConvolutionBackward0]
	140478453633568 -> 140478453633472
	140478453633568 [label=ConvolutionBackward0]
	140478453633760 -> 140478453633568
	140478453633760 [label=ReluBackward0]
	140478453633952 -> 140478453633760
	140478453633952 [label=ConvolutionBackward0]
	140478453634048 -> 140478453633952
	140478453634048 [label=ConvolutionBackward0]
	140478453634240 -> 140478453634048
	140478453634240 [label=ReluBackward0]
	140478453634432 -> 140478453634240
	140478453634432 [label=ConvolutionBackward0]
	140478453634528 -> 140478453634432
	140478453634528 [label=ConvolutionBackward0]
	140478453634720 -> 140478453634528
	140478453634720 [label=ReluBackward0]
	140478453634912 -> 140478453634720
	140478453634912 [label=ConvolutionBackward0]
	140478453635008 -> 140478453634912
	140478453635008 [label=ConvolutionBackward0]
	140478453635200 -> 140478453635008
	140478453635200 [label=ReluBackward0]
	140478453635392 -> 140478453635200
	140478453635392 [label=ConvolutionBackward0]
	140478453635488 -> 140478453635392
	140478453635488 [label=ConvolutionBackward0]
	140478453635680 -> 140478453635488
	140478453635680 [label=ReluBackward0]
	140478453635872 -> 140478453635680
	140478453635872 [label=ConvolutionBackward0]
	140478453635968 -> 140478453635872
	140478453766720 [label="conv1.weight
 (32, 3, 3)" fillcolor=lightblue]
	140478453766720 -> 140478453635968
	140478453635968 [label=AccumulateGrad]
	140478453635920 -> 140478453635872
	140478453766640 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140478453766640 -> 140478453635920
	140478453635920 [label=AccumulateGrad]
	140478453635632 -> 140478453635488
	140478506197136 [label="layers.0.0.depthwise.weight
 (32, 1, 3)" fillcolor=lightblue]
	140478506197136 -> 140478453635632
	140478453635632 [label=AccumulateGrad]
	140478453635584 -> 140478453635488
	140478506215840 [label="layers.0.0.depthwise.bias
 (32)" fillcolor=lightblue]
	140478506215840 -> 140478453635584
	140478453635584 [label=AccumulateGrad]
	140478453635440 -> 140478453635392
	140478474210256 [label="layers.0.0.pointwise.weight
 (64, 32, 1)" fillcolor=lightblue]
	140478474210256 -> 140478453635440
	140478453635440 [label=AccumulateGrad]
	140478453635296 -> 140478453635392
	140478453766800 [label="layers.0.0.pointwise.bias
 (64)" fillcolor=lightblue]
	140478453766800 -> 140478453635296
	140478453635296 [label=AccumulateGrad]
	140478453635152 -> 140478453635008
	140478453766880 [label="layers.0.2.depthwise.weight
 (64, 1, 3)" fillcolor=lightblue]
	140478453766880 -> 140478453635152
	140478453635152 [label=AccumulateGrad]
	140478453635104 -> 140478453635008
	140478453766960 [label="layers.0.2.depthwise.bias
 (64)" fillcolor=lightblue]
	140478453766960 -> 140478453635104
	140478453635104 [label=AccumulateGrad]
	140478453634960 -> 140478453634912
	140478453767040 [label="layers.0.2.pointwise.weight
 (64, 64, 1)" fillcolor=lightblue]
	140478453767040 -> 140478453634960
	140478453634960 [label=AccumulateGrad]
	140478453634816 -> 140478453634912
	140478453767120 [label="layers.0.2.pointwise.bias
 (64)" fillcolor=lightblue]
	140478453767120 -> 140478453634816
	140478453634816 [label=AccumulateGrad]
	140478453634672 -> 140478453634528
	140478473774000 [label="layers.1.0.depthwise.weight
 (64, 1, 3)" fillcolor=lightblue]
	140478473774000 -> 140478453634672
	140478453634672 [label=AccumulateGrad]
	140478453634624 -> 140478453634528
	140478453767200 [label="layers.1.0.depthwise.bias
 (64)" fillcolor=lightblue]
	140478453767200 -> 140478453634624
	140478453634624 [label=AccumulateGrad]
	140478453634480 -> 140478453634432
	140478453767280 [label="layers.1.0.pointwise.weight
 (128, 64, 1)" fillcolor=lightblue]
	140478453767280 -> 140478453634480
	140478453634480 [label=AccumulateGrad]
	140478453634336 -> 140478453634432
	140478453767360 [label="layers.1.0.pointwise.bias
 (128)" fillcolor=lightblue]
	140478453767360 -> 140478453634336
	140478453634336 [label=AccumulateGrad]
	140478453634192 -> 140478453634048
	140478453767440 [label="layers.1.2.depthwise.weight
 (128, 1, 3)" fillcolor=lightblue]
	140478453767440 -> 140478453634192
	140478453634192 [label=AccumulateGrad]
	140478453634144 -> 140478453634048
	140478453767520 [label="layers.1.2.depthwise.bias
 (128)" fillcolor=lightblue]
	140478453767520 -> 140478453634144
	140478453634144 [label=AccumulateGrad]
	140478453634000 -> 140478453633952
	140478453767600 [label="layers.1.2.pointwise.weight
 (128, 128, 1)" fillcolor=lightblue]
	140478453767600 -> 140478453634000
	140478453634000 [label=AccumulateGrad]
	140478453633856 -> 140478453633952
	140478453767680 [label="layers.1.2.pointwise.bias
 (128)" fillcolor=lightblue]
	140478453767680 -> 140478453633856
	140478453633856 [label=AccumulateGrad]
	140478453633712 -> 140478453633568
	140478453767760 [label="layers.2.0.depthwise.weight
 (128, 1, 3)" fillcolor=lightblue]
	140478453767760 -> 140478453633712
	140478453633712 [label=AccumulateGrad]
	140478453633664 -> 140478453633568
	140478453767840 [label="layers.2.0.depthwise.bias
 (128)" fillcolor=lightblue]
	140478453767840 -> 140478453633664
	140478453633664 [label=AccumulateGrad]
	140478453633520 -> 140478453633472
	140478453767920 [label="layers.2.0.pointwise.weight
 (128, 128, 1)" fillcolor=lightblue]
	140478453767920 -> 140478453633520
	140478453633520 [label=AccumulateGrad]
	140478453633088 -> 140478453633472
	140478453768000 [label="layers.2.0.pointwise.bias
 (128)" fillcolor=lightblue]
	140478453768000 -> 140478453633088
	140478453633088 [label=AccumulateGrad]
	140478453735088 -> 140478453735376
	140478453768080 [label="layers.2.2.depthwise.weight
 (128, 1, 3)" fillcolor=lightblue]
	140478453768080 -> 140478453735088
	140478453735088 [label=AccumulateGrad]
	140478453633184 -> 140478453735376
	140478453891136 [label="layers.2.2.depthwise.bias
 (128)" fillcolor=lightblue]
	140478453891136 -> 140478453633184
	140478453633184 [label=AccumulateGrad]
	140478453735328 -> 140478453735280
	140478453891216 [label="layers.2.2.pointwise.weight
 (128, 128, 1)" fillcolor=lightblue]
	140478453891216 -> 140478453735328
	140478453735328 [label=AccumulateGrad]
	140478453735136 -> 140478453735280
	140478453891296 [label="layers.2.2.pointwise.bias
 (128)" fillcolor=lightblue]
	140478453891296 -> 140478453735136
	140478453735136 [label=AccumulateGrad]
	140478453734512 -> 140478453734752
	140478453891376 [label="layers.3.0.depthwise.weight
 (128, 1, 3)" fillcolor=lightblue]
	140478453891376 -> 140478453734512
	140478453734512 [label=AccumulateGrad]
	140478453734656 -> 140478453734752
	140478453891456 [label="layers.3.0.depthwise.bias
 (128)" fillcolor=lightblue]
	140478453891456 -> 140478453734656
	140478453734656 [label=AccumulateGrad]
	140478453734800 -> 140478453575840
	140478453891536 [label="layers.3.0.pointwise.weight
 (256, 128, 1)" fillcolor=lightblue]
	140478453891536 -> 140478453734800
	140478453734800 [label=AccumulateGrad]
	140478453734848 -> 140478453575840
	140478453891616 [label="layers.3.0.pointwise.bias
 (256)" fillcolor=lightblue]
	140478453891616 -> 140478453734848
	140478453734848 [label=AccumulateGrad]
	140478453576416 -> 140478453576848
	140478453891696 [label="layers.3.2.depthwise.weight
 (256, 1, 3)" fillcolor=lightblue]
	140478453891696 -> 140478453576416
	140478453576416 [label=AccumulateGrad]
	140478453576560 -> 140478453576848
	140478453891776 [label="layers.3.2.depthwise.bias
 (256)" fillcolor=lightblue]
	140478453891776 -> 140478453576560
	140478453576560 [label=AccumulateGrad]
	140478453576992 -> 140478453577136
	140478453891856 [label="layers.3.2.pointwise.weight
 (256, 256, 1)" fillcolor=lightblue]
	140478453891856 -> 140478453576992
	140478453576992 [label=AccumulateGrad]
	140478453577424 -> 140478453577136
	140478453891936 [label="layers.3.2.pointwise.bias
 (256)" fillcolor=lightblue]
	140478453891936 -> 140478453577424
	140478453577424 [label=AccumulateGrad]
	140478453577856 -> 140478453578288
	140478453892016 [label="layers.4.0.depthwise.weight
 (256, 1, 3)" fillcolor=lightblue]
	140478453892016 -> 140478453577856
	140478453577856 [label=AccumulateGrad]
	140478453578000 -> 140478453578288
	140478453892096 [label="layers.4.0.depthwise.bias
 (256)" fillcolor=lightblue]
	140478453892096 -> 140478453578000
	140478453578000 [label=AccumulateGrad]
	140478453578432 -> 140478453578576
	140478453892176 [label="layers.4.0.pointwise.weight
 (256, 256, 1)" fillcolor=lightblue]
	140478453892176 -> 140478453578432
	140478453578432 [label=AccumulateGrad]
	140478453578864 -> 140478453578576
	140478453892256 [label="layers.4.0.pointwise.bias
 (256)" fillcolor=lightblue]
	140478453892256 -> 140478453578864
	140478453578864 [label=AccumulateGrad]
	140478453579296 -> 140478453579104
	140478453892336 [label="layers.4.2.depthwise.weight
 (256, 1, 3)" fillcolor=lightblue]
	140478453892336 -> 140478453579296
	140478453579296 [label=AccumulateGrad]
	140478453579248 -> 140478453579104
	140478453892416 [label="layers.4.2.depthwise.bias
 (256)" fillcolor=lightblue]
	140478453892416 -> 140478453579248
	140478453579248 [label=AccumulateGrad]
	140478453579056 -> 140478453578960
	140478453892496 [label="layers.4.2.pointwise.weight
 (256, 256, 1)" fillcolor=lightblue]
	140478453892496 -> 140478453579056
	140478453579056 [label=AccumulateGrad]
	140478453578816 -> 140478453578960
	140478453892576 [label="layers.4.2.pointwise.bias
 (256)" fillcolor=lightblue]
	140478453892576 -> 140478453578816
	140478453578816 [label=AccumulateGrad]
	140478453578624 -> 140478453578384
	140478453892656 [label="layers.5.0.depthwise.weight
 (256, 1, 3)" fillcolor=lightblue]
	140478453892656 -> 140478453578624
	140478453578624 [label=AccumulateGrad]
	140478453578528 -> 140478453578384
	140478453892736 [label="layers.5.0.depthwise.bias
 (256)" fillcolor=lightblue]
	140478453892736 -> 140478453578528
	140478453578528 [label=AccumulateGrad]
	140478453578336 -> 140478453578240
	140478453892816 [label="layers.5.0.pointwise.weight
 (512, 256, 1)" fillcolor=lightblue]
	140478453892816 -> 140478453578336
	140478453578336 [label=AccumulateGrad]
	140478453578096 -> 140478453578240
	140478453892896 [label="layers.5.0.pointwise.bias
 (512)" fillcolor=lightblue]
	140478453892896 -> 140478453578096
	140478453578096 [label=AccumulateGrad]
	140478453577904 -> 140478453577664
	140478453892976 [label="layers.5.2.depthwise.weight
 (512, 1, 3)" fillcolor=lightblue]
	140478453892976 -> 140478453577904
	140478453577904 [label=AccumulateGrad]
	140478453577808 -> 140478453577664
	140478453893056 [label="layers.5.2.depthwise.bias
 (512)" fillcolor=lightblue]
	140478453893056 -> 140478453577808
	140478453577808 [label=AccumulateGrad]
	140478453577616 -> 140478453577520
	140478453893136 [label="layers.5.2.pointwise.weight
 (512, 512, 1)" fillcolor=lightblue]
	140478453893136 -> 140478453577616
	140478453577616 [label=AccumulateGrad]
	140478453576944 -> 140478453577520
	140478453893216 [label="layers.5.2.pointwise.bias
 (512)" fillcolor=lightblue]
	140478453893216 -> 140478453576944
	140478453576944 [label=AccumulateGrad]
	140478453576656 -> 140478453576752
	140478453576656 [label=TBackward0]
	140478453577184 -> 140478453576656
	140478486653984 [label="fc.weight
 (3, 512)" fillcolor=lightblue]
	140478486653984 -> 140478453577184
	140478453577184 [label=AccumulateGrad]
	140478453576752 -> 140478471530992
}
