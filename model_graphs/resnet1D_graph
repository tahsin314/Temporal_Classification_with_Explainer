digraph {
	graph [size="59.849999999999994,59.849999999999994"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140344001311824 [label="
 (4, 10)" fillcolor=darkolivegreen1]
	140344001360368 [label=AddmmBackward0]
	140344001360512 -> 140344001360368
	140344025504960 [label="fc.bias
 (10)" fillcolor=lightblue]
	140344025504960 -> 140344001360512
	140344001360512 [label=AccumulateGrad]
	140344001360416 -> 140344001360368
	140344001360416 [label=ViewBackward0]
	140344001360224 -> 140344001360416
	140344001360224 [label=SqueezeBackward1]
	140344001360704 -> 140344001360224
	140344001360704 [label=MeanBackward1]
	140344001360848 -> 140344001360704
	140344001360848 [label=UnsqueezeBackward0]
	140344001360992 -> 140344001360848
	140344001360992 [label=ReluBackward0]
	140344001361136 -> 140344001360992
	140344001361136 [label=AddBackward0]
	140344001361280 -> 140344001361136
	140344001361280 [label=NativeBatchNormBackward0]
	140344001361520 -> 140344001361280
	140344001361520 [label=ConvolutionBackward0]
	140344001361616 -> 140344001361520
	140344001361616 [label=ReluBackward0]
	140344001361184 -> 140344001361616
	140344001361184 [label=NativeBatchNormBackward0]
	140344001360896 -> 140344001361184
	140344001360896 [label=ConvolutionBackward0]
	140344001361232 -> 140344001360896
	140344001361232 [label=ReluBackward0]
	140344001360032 -> 140344001361232
	140344001360032 [label=AddBackward0]
	140344001363152 -> 140344001360032
	140344001363152 [label=NativeBatchNormBackward0]
	140344001363008 -> 140344001363152
	140344001363008 [label=ConvolutionBackward0]
	140344001362816 -> 140344001363008
	140344001362816 [label=ReluBackward0]
	140344001362672 -> 140344001362816
	140344001362672 [label=NativeBatchNormBackward0]
	140344001362576 -> 140344001362672
	140344001362576 [label=ConvolutionBackward0]
	140344001362384 -> 140344001362576
	140344001362384 [label=ReluBackward0]
	140344001362240 -> 140344001362384
	140344001362240 [label=AddBackward0]
	140344001362144 -> 140344001362240
	140344001362144 [label=NativeBatchNormBackward0]
	140344001362000 -> 140344001362144
	140344001362000 [label=ConvolutionBackward0]
	140344001361808 -> 140344001362000
	140344001361808 [label=ReluBackward0]
	140344001363872 -> 140344001361808
	140344001363872 [label=NativeBatchNormBackward0]
	140344001363920 -> 140344001363872
	140344001363920 [label=ConvolutionBackward0]
	140344001362192 -> 140344001363920
	140344001362192 [label=ReluBackward0]
	140344001343840 -> 140344001362192
	140344001343840 [label=AddBackward0]
	140344001343936 -> 140344001343840
	140344001343936 [label=NativeBatchNormBackward0]
	140344001344080 -> 140344001343936
	140344001344080 [label=ConvolutionBackward0]
	140344001344272 -> 140344001344080
	140344001344272 [label=ReluBackward0]
	140344001344416 -> 140344001344272
	140344001344416 [label=NativeBatchNormBackward0]
	140344001344512 -> 140344001344416
	140344001344512 [label=ConvolutionBackward0]
	140344001344704 -> 140344001344512
	140344001344704 [label=ReluBackward0]
	140344001344848 -> 140344001344704
	140344001344848 [label=AddBackward0]
	140344001344944 -> 140344001344848
	140344001344944 [label=NativeBatchNormBackward0]
	140344001345088 -> 140344001344944
	140344001345088 [label=ConvolutionBackward0]
	140344001345280 -> 140344001345088
	140344001345280 [label=ReluBackward0]
	140344001345424 -> 140344001345280
	140344001345424 [label=NativeBatchNormBackward0]
	140344001345520 -> 140344001345424
	140344001345520 [label=ConvolutionBackward0]
	140344001344896 -> 140344001345520
	140344001344896 [label=ReluBackward0]
	140344001345808 -> 140344001344896
	140344001345808 [label=AddBackward0]
	140344001345904 -> 140344001345808
	140344001345904 [label=NativeBatchNormBackward0]
	140344001346048 -> 140344001345904
	140344001346048 [label=ConvolutionBackward0]
	140344001346240 -> 140344001346048
	140344001346240 [label=ReluBackward0]
	140344001346384 -> 140344001346240
	140344001346384 [label=NativeBatchNormBackward0]
	140344001346480 -> 140344001346384
	140344001346480 [label=ConvolutionBackward0]
	140344001346672 -> 140344001346480
	140344001346672 [label=ReluBackward0]
	140344001346816 -> 140344001346672
	140344001346816 [label=AddBackward0]
	140344001346912 -> 140344001346816
	140344001346912 [label=NativeBatchNormBackward0]
	140344001347056 -> 140344001346912
	140344001347056 [label=ConvolutionBackward0]
	140344001347248 -> 140344001347056
	140344001347248 [label=ReluBackward0]
	140344001347392 -> 140344001347248
	140344001347392 [label=NativeBatchNormBackward0]
	140344001347488 -> 140344001347392
	140344001347488 [label=ConvolutionBackward0]
	140344001346864 -> 140344001347488
	140344001346864 [label=ReluBackward0]
	140344001339648 -> 140344001346864
	140344001339648 [label=AddBackward0]
	140344001339744 -> 140344001339648
	140344001339744 [label=NativeBatchNormBackward0]
	140344001339888 -> 140344001339744
	140344001339888 [label=ConvolutionBackward0]
	140344001340080 -> 140344001339888
	140344001340080 [label=ReluBackward0]
	140344001340224 -> 140344001340080
	140344001340224 [label=NativeBatchNormBackward0]
	140344001340320 -> 140344001340224
	140344001340320 [label=ConvolutionBackward0]
	140344001339696 -> 140344001340320
	140344001339696 [label=ReluBackward0]
	140344001340608 -> 140344001339696
	140344001340608 [label=NativeBatchNormBackward0]
	140344001340704 -> 140344001340608
	140344001340704 [label=ConvolutionBackward0]
	140344001340896 -> 140344001340704
	140344001479840 [label="conv1.weight
 (64, 3, 7)" fillcolor=lightblue]
	140344001479840 -> 140344001340896
	140344001340896 [label=AccumulateGrad]
	140344001340656 -> 140344001340608
	140344053832176 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140344053832176 -> 140344001340656
	140344001340656 [label=AccumulateGrad]
	140344001340416 -> 140344001340608
	140344018838128 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140344018838128 -> 140344001340416
	140344001340416 [label=AccumulateGrad]
	140344001340512 -> 140344001340320
	140344001480240 [label="layer1.0.conv1.weight
 (64, 64, 3)" fillcolor=lightblue]
	140344001480240 -> 140344001340512
	140344001340512 [label=AccumulateGrad]
	140344001340272 -> 140344001340224
	140344001480320 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140344001480320 -> 140344001340272
	140344001340272 [label=AccumulateGrad]
	140344001340128 -> 140344001340224
	140344001480400 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140344001480400 -> 140344001340128
	140344001340128 [label=AccumulateGrad]
	140344001340032 -> 140344001339888
	140344001480720 [label="layer1.0.conv2.weight
 (64, 64, 3)" fillcolor=lightblue]
	140344001480720 -> 140344001340032
	140344001340032 [label=AccumulateGrad]
	140344001339840 -> 140344001339744
	140344001480800 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140344001480800 -> 140344001339840
	140344001339840 [label=AccumulateGrad]
	140344001339792 -> 140344001339744
	140344001480880 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140344001480880 -> 140344001339792
	140344001339792 [label=AccumulateGrad]
	140344001339696 -> 140344001339648
	140344001339552 -> 140344001347488
	140344001481120 [label="layer1.1.conv1.weight
 (64, 64, 3)" fillcolor=lightblue]
	140344001481120 -> 140344001339552
	140344001339552 [label=AccumulateGrad]
	140344001347440 -> 140344001347392
	140344001481200 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140344001481200 -> 140344001347440
	140344001347440 [label=AccumulateGrad]
	140344001347296 -> 140344001347392
	140344001481280 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140344001481280 -> 140344001347296
	140344001347296 [label=AccumulateGrad]
	140344001347200 -> 140344001347056
	140344001481600 [label="layer1.1.conv2.weight
 (64, 64, 3)" fillcolor=lightblue]
	140344001481600 -> 140344001347200
	140344001347200 [label=AccumulateGrad]
	140344001347008 -> 140344001346912
	140344001481680 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140344001481680 -> 140344001347008
	140344001347008 [label=AccumulateGrad]
	140344001346960 -> 140344001346912
	140344001481760 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140344001481760 -> 140344001346960
	140344001346960 [label=AccumulateGrad]
	140344001346864 -> 140344001346816
	140344001346624 -> 140344001346480
	140344001482080 [label="layer2.0.conv1.weight
 (128, 64, 3)" fillcolor=lightblue]
	140344001482080 -> 140344001346624
	140344001346624 [label=AccumulateGrad]
	140344001346432 -> 140344001346384
	140344001482160 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140344001482160 -> 140344001346432
	140344001346432 [label=AccumulateGrad]
	140344001346288 -> 140344001346384
	140344001482240 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140344001482240 -> 140344001346288
	140344001346288 [label=AccumulateGrad]
	140344001346192 -> 140344001346048
	140344001482560 [label="layer2.0.conv2.weight
 (128, 128, 3)" fillcolor=lightblue]
	140344001482560 -> 140344001346192
	140344001346192 [label=AccumulateGrad]
	140344001346000 -> 140344001345904
	140344001482640 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140344001482640 -> 140344001346000
	140344001346000 [label=AccumulateGrad]
	140344001345952 -> 140344001345904
	140344001593408 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140344001593408 -> 140344001345952
	140344001345952 [label=AccumulateGrad]
	140344001345856 -> 140344001345808
	140344001345856 [label=NativeBatchNormBackward0]
	140344001346576 -> 140344001345856
	140344001346576 [label=ConvolutionBackward0]
	140344001346672 -> 140344001346576
	140344001346720 -> 140344001346576
	140344001593728 [label="layer2.0.shortcut.0.weight
 (128, 64, 1)" fillcolor=lightblue]
	140344001593728 -> 140344001346720
	140344001346720 [label=AccumulateGrad]
	140344001346144 -> 140344001345856
	140344001593808 [label="layer2.0.shortcut.1.weight
 (128)" fillcolor=lightblue]
	140344001593808 -> 140344001346144
	140344001346144 [label=AccumulateGrad]
	140344001346096 -> 140344001345856
	140344001593888 [label="layer2.0.shortcut.1.bias
 (128)" fillcolor=lightblue]
	140344001593888 -> 140344001346096
	140344001346096 [label=AccumulateGrad]
	140344001345712 -> 140344001345520
	140344001594208 [label="layer2.1.conv1.weight
 (128, 128, 3)" fillcolor=lightblue]
	140344001594208 -> 140344001345712
	140344001345712 [label=AccumulateGrad]
	140344001345472 -> 140344001345424
	140344001594288 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140344001594288 -> 140344001345472
	140344001345472 [label=AccumulateGrad]
	140344001345328 -> 140344001345424
	140344001594368 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140344001594368 -> 140344001345328
	140344001345328 [label=AccumulateGrad]
	140344001345232 -> 140344001345088
	140344001594688 [label="layer2.1.conv2.weight
 (128, 128, 3)" fillcolor=lightblue]
	140344001594688 -> 140344001345232
	140344001345232 [label=AccumulateGrad]
	140344001345040 -> 140344001344944
	140344001594768 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140344001594768 -> 140344001345040
	140344001345040 [label=AccumulateGrad]
	140344001344992 -> 140344001344944
	140344001594848 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140344001594848 -> 140344001344992
	140344001344992 [label=AccumulateGrad]
	140344001344896 -> 140344001344848
	140344001344656 -> 140344001344512
	140344001595168 [label="layer3.0.conv1.weight
 (256, 128, 3)" fillcolor=lightblue]
	140344001595168 -> 140344001344656
	140344001344656 [label=AccumulateGrad]
	140344001344464 -> 140344001344416
	140344001595248 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140344001595248 -> 140344001344464
	140344001344464 [label=AccumulateGrad]
	140344001344320 -> 140344001344416
	140344001595328 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140344001595328 -> 140344001344320
	140344001344320 [label=AccumulateGrad]
	140344001344224 -> 140344001344080
	140344001595648 [label="layer3.0.conv2.weight
 (256, 256, 3)" fillcolor=lightblue]
	140344001595648 -> 140344001344224
	140344001344224 [label=AccumulateGrad]
	140344001344032 -> 140344001343936
	140344001595728 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140344001595728 -> 140344001344032
	140344001344032 [label=AccumulateGrad]
	140344001343984 -> 140344001343936
	140344001595808 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140344001595808 -> 140344001343984
	140344001343984 [label=AccumulateGrad]
	140344001343888 -> 140344001343840
	140344001343888 [label=NativeBatchNormBackward0]
	140344001344608 -> 140344001343888
	140344001344608 [label=ConvolutionBackward0]
	140344001344704 -> 140344001344608
	140344001344752 -> 140344001344608
	140344001596128 [label="layer3.0.shortcut.0.weight
 (256, 128, 1)" fillcolor=lightblue]
	140344001596128 -> 140344001344752
	140344001344752 [label=AccumulateGrad]
	140344001344176 -> 140344001343888
	140344001596208 [label="layer3.0.shortcut.1.weight
 (256)" fillcolor=lightblue]
	140344001596208 -> 140344001344176
	140344001344176 [label=AccumulateGrad]
	140344001344128 -> 140344001343888
	140344001596288 [label="layer3.0.shortcut.1.bias
 (256)" fillcolor=lightblue]
	140344001596288 -> 140344001344128
	140344001344128 [label=AccumulateGrad]
	140344001343744 -> 140344001363920
	140344001596608 [label="layer3.1.conv1.weight
 (256, 256, 3)" fillcolor=lightblue]
	140344001596608 -> 140344001343744
	140344001343744 [label=AccumulateGrad]
	140344001363728 -> 140344001363872
	140344001596688 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140344001596688 -> 140344001363728
	140344001363728 [label=AccumulateGrad]
	140344001343552 -> 140344001363872
	140344001596768 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140344001596768 -> 140344001343552
	140344001343552 [label=AccumulateGrad]
	140344001361856 -> 140344001362000
	140344001597088 [label="layer3.1.conv2.weight
 (256, 256, 3)" fillcolor=lightblue]
	140344001597088 -> 140344001361856
	140344001361856 [label=AccumulateGrad]
	140344001362048 -> 140344001362144
	140344001597168 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140344001597168 -> 140344001362048
	140344001362048 [label=AccumulateGrad]
	140344001362096 -> 140344001362144
	140344001597248 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140344001597248 -> 140344001362096
	140344001362096 [label=AccumulateGrad]
	140344001362192 -> 140344001362240
	140344001362432 -> 140344001362576
	140344001200352 [label="layer4.0.conv1.weight
 (512, 256, 3)" fillcolor=lightblue]
	140344001200352 -> 140344001362432
	140344001362432 [label=AccumulateGrad]
	140344001362624 -> 140344001362672
	140344001200432 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140344001200432 -> 140344001362624
	140344001362624 [label=AccumulateGrad]
	140344001362768 -> 140344001362672
	140344001200512 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140344001200512 -> 140344001362768
	140344001362768 [label=AccumulateGrad]
	140344001362864 -> 140344001363008
	140344001200832 [label="layer4.0.conv2.weight
 (512, 512, 3)" fillcolor=lightblue]
	140344001200832 -> 140344001362864
	140344001362864 [label=AccumulateGrad]
	140344001363056 -> 140344001363152
	140344001200912 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140344001200912 -> 140344001363056
	140344001363056 [label=AccumulateGrad]
	140344001363104 -> 140344001363152
	140344001200992 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140344001200992 -> 140344001363104
	140344001363104 [label=AccumulateGrad]
	140344001363200 -> 140344001360032
	140344001363200 [label=NativeBatchNormBackward0]
	140344001362480 -> 140344001363200
	140344001362480 [label=ConvolutionBackward0]
	140344001362384 -> 140344001362480
	140344001362336 -> 140344001362480
	140344001201312 [label="layer4.0.shortcut.0.weight
 (512, 256, 1)" fillcolor=lightblue]
	140344001201312 -> 140344001362336
	140344001362336 [label=AccumulateGrad]
	140344001362912 -> 140344001363200
	140344001201392 [label="layer4.0.shortcut.1.weight
 (512)" fillcolor=lightblue]
	140344001201392 -> 140344001362912
	140344001362912 [label=AccumulateGrad]
	140344001362960 -> 140344001363200
	140344001201472 [label="layer4.0.shortcut.1.bias
 (512)" fillcolor=lightblue]
	140344001201472 -> 140344001362960
	140344001362960 [label=AccumulateGrad]
	140344001360320 -> 140344001360896
	140344001201792 [label="layer4.1.conv1.weight
 (512, 512, 3)" fillcolor=lightblue]
	140344001201792 -> 140344001360320
	140344001360320 [label=AccumulateGrad]
	140344001361040 -> 140344001361184
	140344001201872 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140344001201872 -> 140344001361040
	140344001361040 [label=AccumulateGrad]
	140344001361472 -> 140344001361184
	140344001201952 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140344001201952 -> 140344001361472
	140344001361472 [label=AccumulateGrad]
	140344001361712 -> 140344001361520
	140344001202272 [label="layer4.1.conv2.weight
 (512, 512, 3)" fillcolor=lightblue]
	140344001202272 -> 140344001361712
	140344001361712 [label=AccumulateGrad]
	140344001361424 -> 140344001361280
	140344001202352 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140344001202352 -> 140344001361424
	140344001361424 [label=AccumulateGrad]
	140344001361376 -> 140344001361280
	140344001202432 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140344001202432 -> 140344001361376
	140344001361376 [label=AccumulateGrad]
	140344001361232 -> 140344001361136
	140344001360272 -> 140344001360368
	140344001360272 [label=TBackward0]
	140344001360800 -> 140344001360272
	140344026880096 [label="fc.weight
 (10, 512)" fillcolor=lightblue]
	140344026880096 -> 140344001360800
	140344001360800 [label=AccumulateGrad]
	140344001360368 -> 140344001311824
}
